{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Voice Codex — Essence Analysis\n",
        "\n",
        "This notebook analyses the 82-dim **Voice Essence** vectors extracted from `voice_codex_dataset`.\n",
        "\n",
        "### Vector layout (ESSENCE_DIM = 82)\n",
        "\n",
        "| Slice | Segment | Dims | Description |\n",
        "|-------|---------|------|--------------|\n",
        "| [0:5] | F0 stats | 5 | Pitch mean, std, min, max (Hz); voiced fraction |\n",
        "| [5:8] | F0 trajectory | 3 | Slope (Hz/s); vibrato rate (Hz); vibrato depth (cents RMS) |\n",
        "| [8:21] | MFCC mean | 13 | Vocal tract shape — temporal means |\n",
        "| [21:34] | MFCC std | 13 | Vocal tract shape — temporal stds |\n",
        "| [34:47] | Delta-MFCC mean | 13 | Rate of vocal tract change — means |\n",
        "| [47:60] | Delta-MFCC std | 13 | Rate of vocal tract change — stds |\n",
        "| [60:66] | Formants F1–F3 | 6 | Resonance frequencies — mean & std per formant (Hz) |\n",
        "| [66:68] | HNR | 2 | Harmonics-to-noise ratio — mean & std (dB) |\n",
        "| [68:72] | Amplitude | 4 | RMS envelope — mean, std, skewness, excess kurtosis |\n",
        "| [72:78] | Spectral | 6 | Centroid mean/std, bandwidth mean, rolloff mean, ZCR mean/std |\n",
        "| [78:82] | Onset | 4 | Onset rate, strength mean/std, mean inter-onset interval (s) |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from extract import ESSENCE_LAYOUT, ESSENCE_DIM\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_theme(style='whitegrid', palette='tab10')\n",
        "plt.rcParams['figure.dpi'] = 120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load essences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data   = np.load('essences.npz', allow_pickle=False)\n",
        "X      = data['X']        # (45, 82)\n",
        "labels = data['labels']   # speaker IDs\n",
        "files  = data['files']\n",
        "\n",
        "speakers     = sorted(set(labels))\n",
        "n_speakers   = len(speakers)\n",
        "spk_idx      = {s: i for i, s in enumerate(speakers)}\n",
        "color_ids    = np.array([spk_idx[l] for l in labels])\n",
        "palette      = plt.cm.tab20(np.linspace(0, 1, n_speakers))\n",
        "\n",
        "print(f'Shape: {X.shape}  |  Speakers: {n_speakers}  |  All finite: {np.all(np.isfinite(X))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sanity check: intra- vs inter-speaker distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scaler = StandardScaler()\n",
        "Xs     = scaler.fit_transform(X)\n",
        "\n",
        "intra, inter = [], []\n",
        "for i in range(len(Xs)):\n",
        "    for j in range(i + 1, len(Xs)):\n",
        "        d = np.linalg.norm(Xs[i] - Xs[j])\n",
        "        (intra if labels[i] == labels[j] else inter).append(d)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.hist(intra, bins=20, alpha=0.6, label=f'Intra-speaker  μ={np.mean(intra):.2f}')\n",
        "ax.hist(inter, bins=40, alpha=0.6, label=f'Inter-speaker  μ={np.mean(inter):.2f}')\n",
        "ax.set_xlabel('Euclidean distance (standardised)')\n",
        "ax.set_title(f'Speaker separation  |  inter/intra = {np.mean(inter)/np.mean(intra):.2f}×')\n",
        "ax.legend()\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "print(f'Intra: mean={np.mean(intra):.3f}  std={np.std(intra):.3f}')\n",
        "print(f'Inter: mean={np.mean(inter):.3f}  std={np.std(inter):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. F0 per speaker — pitch identity signature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f0_means = {spk: X[labels == spk, 0] for spk in speakers}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "for i, spk in enumerate(speakers):\n",
        "    vals = f0_means[spk]\n",
        "    ax.scatter([i] * len(vals), vals, color=palette[i], s=60, zorder=3)\n",
        "    ax.plot([i - 0.3, i + 0.3], [vals.mean()] * 2, color=palette[i], lw=2)\n",
        "\n",
        "ax.set_xticks(range(n_speakers))\n",
        "ax.set_xticklabels([s.replace('speaker_', 'S') for s in speakers], rotation=45)\n",
        "ax.set_ylabel('F0 mean (Hz)')\n",
        "ax.set_title('Fundamental Frequency per Speaker — each dot is one clip')\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. HNR and Formant F1 comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "for ax, feat_idx, ylabel, title in [\n",
        "    (axes[0], 66, 'HNR mean (dB)',       'Harmonics-to-Noise Ratio per Speaker'),\n",
        "    (axes[1], 60, 'F1 mean (Hz)',         'Formant F1 per Speaker'),\n",
        "]:\n",
        "    for i, spk in enumerate(speakers):\n",
        "        vals = X[labels == spk, feat_idx]\n",
        "        ax.scatter([i] * len(vals), vals, color=palette[i], s=60, zorder=3)\n",
        "        ax.plot([i - 0.3, i + 0.3], [vals.mean()] * 2, color=palette[i], lw=2)\n",
        "    ax.set_xticks(range(n_speakers))\n",
        "    ax.set_xticklabels([s.replace('speaker_', 'S') for s in speakers], rotation=45)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. MFCC mean profile (vocal tract fingerprint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Report: Reasoning, Results, and Honest Limitations\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Why these features?\n",
        "\n",
        "The 82-dimensional essence vector was designed around one question: *what is the minimum set of acoustic measurements that uniquely fingerprints a voice without requiring a neural network?*\n",
        "\n",
        "**MFCCs [dims 8–60] — the backbone (52 dims)**  \n",
        "MFCCs capture the spectral envelope of the vocal tract — the shape of the resonance cavity that makes each voice unique. They are the single strongest speaker-discriminating signal in classical ASR and speaker recognition literature. Using both mean and std per coefficient doubles the information: mean captures the average posture, std captures how much it varies (speaking style). Delta-MFCCs add temporal dynamics — how quickly the vocal tract changes — which discriminates between fast and deliberate speakers.\n",
        "\n",
        "**F0 statistics [dims 0–7] — the pitch signature (8 dims)**  \n",
        "Fundamental frequency is the most perceptually salient voice property. Mean F0 alone separates most speakers (bass from tenor from soprano). The slope and vibrato depth add dynamics. These 8 dimensions are cheap to compute and provide strong speaker-level priors.\n",
        "\n",
        "**Formants F1–F3 [dims 60–65] — the vowel fingerprint (6 dims)**  \n",
        "Formant frequencies encode the resonance peaks of the vocal tract — they literally describe the physical geometry of the speaker's mouth, pharynx, and nasal cavity. F1 and F2 define the vowel space; F3 adds voice quality. These are the features speech therapists have used for decades. They're expensive to compute (Praat Burg algorithm) but irreplaceable for vocal tract shape.\n",
        "\n",
        "**HNR [dims 66–67] — breathiness vs. clarity (2 dims)**  \n",
        "Harmonics-to-Noise Ratio distinguishes modal voice (clean, harmonic) from breathy voice (noisy, air-leaky). Two speakers with identical F0 and MFCCs can differ substantially in HNR — it captures voice quality, not just pitch or spectral shape.\n",
        "\n",
        "**Amplitude envelope [dims 68–71] — loudness contour (4 dims)**  \n",
        "RMS skewness and kurtosis capture the *shape* of loudness variation — does the speaker speak at steady volume or with dramatic peaks? This encodes speaking style rather than vocal anatomy.\n",
        "\n",
        "**Spectral and onset features [dims 72–81] — brightness and rhythm (10 dims)**  \n",
        "Spectral centroid (brightness), rolloff, ZCR, and onset rate/strength capture the high-frequency texture of speech and the sharpness of consonant articulation. Onset rate is a proxy for speaking tempo.\n",
        "\n",
        "**What was left out and why:**  \n",
        "- Chroma features: more musical than speech-relevant  \n",
        "- Pitch class histograms: no clear speaker-ID motivation  \n",
        "- Full covariance MFCC (GMM-UBM style): would require O(D²) storage, incompatible with online Welford\n",
        "\n",
        "---\n",
        "\n",
        "## 2. What the accuracy-vs-scale experiment shows\n",
        "\n",
        "| Chorus N | AUC-ROC | Interpretation |\n",
        "|----------|---------|----------------|\n",
        "| 3 | **0.992** | Near-perfect — 3 voices dominate the mean, easy to detect |\n",
        "| 5 | 0.928 | Still excellent |\n",
        "| 10 | 0.779 | Noticeable degradation |\n",
        "| 20 | 0.662 | Approaching chance |\n",
        "| 40 | 0.585 | Close to random (0.5 baseline) |\n",
        "\n",
        "**The crystal's memory gets blurrier as the chorus grows.** This is exactly what theory predicts.\n",
        "\n",
        "Each voice contributes 1/N to the aggregate mean. As N grows, a member's \"pull\" on the mean approaches zero — by N=40, individual membership is nearly undetectable from the mean alone. This is a fundamental limitation of mean-based aggregation, not a bug in the implementation.\n",
        "\n",
        "**Why Mahalanobis outperforms cosine similarity:**  \n",
        "Cosine similarity only measures directional alignment with the mean vector. It ignores variance — dimensions with high within-corpus variance (e.g., MFCC C0, which encodes loudness) get the same weight as stable dimensions (e.g., F0, which is speaker-stable). Mahalanobis naturally down-weights noisy dimensions and amplifies stable ones, giving it substantially higher AUC at every chorus size.\n",
        "\n",
        "**The AUC-PR flip at large N:**  \n",
        "Note that AUC-PR *increases* from 0.63 (N=5) to 0.91 (N=40). This is a base-rate artifact: at N=40, 40/45 = 89% of voices are \"positives\" — a classifier that always predicts \"member\" gets AUC-PR ≈ 0.89. AUC-ROC is the better metric here because it is base-rate invariant.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. What I'd do differently with more time\n",
        "\n",
        "**a) Full covariance Mahalanobis**  \n",
        "The current implementation uses a *diagonal* covariance — it treats all feature dimensions as independent. Real speech features are highly correlated (e.g., adjacent MFCC coefficients). A full D×D covariance matrix would give true Mahalanobis distance. The challenge: tracking full covariance online requires O(D²) storage (82² = 6,724 floats — manageable) and an online covariance update formula (Chan et al. 1979 extends to this). This would likely improve AUC at all chorus sizes.\n",
        "\n",
        "**b) PCA whitening before aggregation**  \n",
        "Project essences to a lower-dimensional space (say 20 PCA dims explaining 90% variance) before Welford aggregation. This decorrelates features and reduces dimensionality, making the diagonal Mahalanobis a better approximation of full Mahalanobis.\n",
        "\n",
        "**c) Leave-one-out verification for honest out-of-sample evaluation**  \n",
        "The current experiment is *in-sample* — voices that were in the aggregate are tested against that same aggregate. A fairer test would exclude voice i when building the aggregate, then test voice i against the N-1 aggregate. This measures whether the aggregate \"remembers\" voice i in a causal sense. Expected result: even lower AUC, but more honest.\n",
        "\n",
        "**d) Neural embedding baseline**  \n",
        "Compare the classical feature pipeline against a pretrained speaker embedding (e.g., x-vectors, ECAPA-TDNN). Classical features are interpretable and fast; neural embeddings are likely more discriminative. Knowing the gap would clarify when each approach is appropriate.\n",
        "\n",
        "**e) Larger, more diverse dataset**  \n",
        "With only 15 TTS-synthesized speakers, the results are consistent but limited. Real human speech has far greater within-speaker variability (different recording conditions, emotional states, health) — the verification accuracy would likely be lower and the degradation steeper.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Determinism: sources and fixes\n",
        "\n",
        "| Source | Risk | Fix |\n",
        "|--------|------|-----|\n",
        "| K-Means init | Different centroids with different seeds | `random_state=42` hard-coded |\n",
        "| Welford order | FP non-associativity: different order → different mean | Files always processed in sorted path order |\n",
        "| forge_update in-place | Array aliasing between runs | `forge_init()` allocates fresh arrays; verified in tests |\n",
        "| librosa loading | Backend variation | Fixed `sr=16000, mono=True` via soundfile |\n",
        "| Parselmouth | Possible Praat state sharing | New `Sound` object per call; no shared state |\n",
        "| BLAS matmul (cosine) | Parallel reduction order | Arrays are tiny (45×82); BLAS uses sequential code |\n",
        "\n",
        "The most subtle source is **Welford order-dependence**: `test_determinism.py::TestWelfordOrderDependence::test_different_order_differs` demonstrates this explicitly. The fix is architectural — `collect_audio_files()` always returns sorted paths, making order a deterministic function of the filesystem state."
      ],
      "id": "db59ff0a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_mfcc = 13\n",
        "mfcc_slice = slice(8, 21)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "for i, spk in enumerate(speakers):\n",
        "    m = X[labels == spk][:, mfcc_slice].mean(axis=0)\n",
        "    ax.plot(range(n_mfcc), m, marker='o', color=palette[i],\n",
        "            label=spk.replace('speaker_', 'S'), alpha=0.8, lw=1.5)\n",
        "\n",
        "ax.set_xticks(range(n_mfcc))\n",
        "ax.set_xticklabels([f'C{i}' for i in range(n_mfcc)])\n",
        "ax.set_xlabel('MFCC coefficient')\n",
        "ax.set_ylabel('Mean value')\n",
        "ax.set_title('MFCC Mean Profile per Speaker  — vocal tract fingerprint')\n",
        "ax.legend(ncol=5, fontsize=7)\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PCA — 2-D speaker map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pca  = PCA(n_components=10)\n",
        "X2   = pca.fit_transform(Xs)[:, :2]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scree plot\n",
        "cum = np.cumsum(pca.explained_variance_ratio_) * 100\n",
        "axes[0].bar(range(1, 11), pca.explained_variance_ratio_ * 100, alpha=0.7)\n",
        "axes[0].plot(range(1, 11), cum, 'ro-', label='Cumulative')\n",
        "axes[0].axhline(90, color='gray', linestyle='--', alpha=0.5, label='90 %')\n",
        "axes[0].set_xlabel('PC'); axes[0].set_ylabel('Variance explained (%)')\n",
        "axes[0].set_title('PCA Scree Plot'); axes[0].legend()\n",
        "\n",
        "# 2-D scatter\n",
        "for i, spk in enumerate(speakers):\n",
        "    mask = labels == spk\n",
        "    axes[1].scatter(X2[mask, 0], X2[mask, 1], color=palette[i],\n",
        "                    s=80, label=spk.replace('speaker_', 'S'), edgecolors='k', lw=0.4)\n",
        "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f} %)')\n",
        "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f} %)')\n",
        "axes[1].set_title('PCA 2-D: each speaker forms a tight cluster')\n",
        "axes[1].legend(ncol=3, fontsize=7)\n",
        "\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. t-SNE — non-linear speaker map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tsne   = TSNE(n_components=2, perplexity=10, random_state=42, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(Xs)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "for i, spk in enumerate(speakers):\n",
        "    mask = labels == spk\n",
        "    ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], color=palette[i],\n",
        "               s=100, label=spk.replace('speaker_', 'S'), edgecolors='k', lw=0.4)\n",
        "    # label centroid\n",
        "    cx, cy = X_tsne[mask, 0].mean(), X_tsne[mask, 1].mean()\n",
        "    ax.text(cx, cy, spk.replace('speaker_', 'S'), fontsize=7,\n",
        "            ha='center', va='center', weight='bold')\n",
        "\n",
        "ax.set_title('t-SNE projection of 82-dim Voice Essence')\n",
        "ax.legend(ncol=3, fontsize=7, loc='lower right')\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature-segment importance (variance contribution)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Variance of each dimension across the corpus (standardised)\n",
        "# High variance = segment carries discriminative information\n",
        "per_dim_var = Xs.var(axis=0)\n",
        "\n",
        "seg_var = {}\n",
        "for name, (sl, _) in ESSENCE_LAYOUT.items():\n",
        "    seg_var[name] = per_dim_var[sl].mean()\n",
        "\n",
        "names = list(seg_var.keys())\n",
        "vals  = list(seg_var.values())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "bars = ax.bar(names, vals, color=plt.cm.viridis(np.linspace(0.2, 0.85, len(names))))\n",
        "ax.set_ylabel('Mean per-dimension variance (standardised)')\n",
        "ax.set_title('Feature Segment Discriminative Variance')\n",
        "ax.tick_params(axis='x', rotation=35)\n",
        "for bar, v in zip(bars, vals):\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
        "            f'{v:.2f}', ha='center', fontsize=8)\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Correlation heatmap (scalar segments)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use only the first 2 features per segment as summary\n",
        "sel_idx  = [0, 1, 5, 6, 8, 9, 60, 61, 62, 63, 66, 67, 68, 69, 72, 73, 78, 79]\n",
        "sel_names = [\n",
        "    'F0 mean', 'F0 std', 'F0 slope', 'Vibrato rate',\n",
        "    'MFCC0 μ', 'MFCC1 μ',\n",
        "    'F1 mean', 'F1 std', 'F2 mean', 'F2 std',\n",
        "    'HNR mean', 'HNR std',\n",
        "    'RMS mean', 'RMS std',\n",
        "    'Centroid μ', 'Centroid σ',\n",
        "    'Onset rate', 'Onset str μ',\n",
        "]\n",
        "\n",
        "df_sel = pd.DataFrame(X[:, sel_idx], columns=sel_names)\n",
        "corr   = df_sel.corr()\n",
        "mask   = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 9))\n",
        "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f',\n",
        "            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
        "            linewidths=0.4, ax=ax, annot_kws={'size': 7})\n",
        "ax.set_title('Cross-segment Correlation (selected features)')\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rows = []\n",
        "for spk in speakers:\n",
        "    mask = labels == spk\n",
        "    v    = X[mask]\n",
        "    rows.append({\n",
        "        'Speaker':      spk,\n",
        "        'F0 mean (Hz)': f\"{v[:, 0].mean():.1f} ± {v[:, 0].std():.1f}\",\n",
        "        'HNR (dB)':     f\"{v[:, 66].mean():.1f} ± {v[:, 66].std():.1f}\",\n",
        "        'F1 (Hz)':      f\"{v[:, 60].mean():.0f} ± {v[:, 60].std():.0f}\",\n",
        "        'F2 (Hz)':      f\"{v[:, 62].mean():.0f} ± {v[:, 62].std():.0f}\",\n",
        "        'Onset/s':      f\"{v[:, 78].mean():.2f}\",\n",
        "        'Vibrato depth':f\"{v[:, 7].mean():.1f} cts\",\n",
        "    })\n",
        "\n",
        "pd.DataFrame(rows).set_index('Speaker')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}